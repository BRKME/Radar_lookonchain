name: Lookonchain Web Scraper

on:
  schedule:
    - cron: '*/30 * * * *'  # Every 30 minutes
  workflow_dispatch:

permissions:
  contents: write

concurrency:
  group: lookonchain-scraper
  cancel-in-progress: false  # Don't cancel, wait for previous to finish

jobs:
  scrape:
    runs-on: ubuntu-22.04
    timeout-minutes: 10
    
    steps:
      - uses: actions/checkout@v3
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Run scraper
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          TELEGRAM_BOT_TOKEN: ${{ secrets.TELEGRAM_BOT_TOKEN }}
          TELEGRAM_CHAT_ID: ${{ secrets.TELEGRAM_CHAT_ID }}
          ADMIN_CHAT_ID: ${{ secrets.ADMIN_CHAT_ID }}
        run: python main.py
      
      - name: Commit state files
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Pull latest changes first to avoid conflicts
          if ! git pull --rebase; then
            echo "⚠️ Git pull failed, attempting to continue anyway"
            git rebase --abort || true
          fi
          
          # Add state files
          git add -f last_feed_id.txt processed_hashes.txt
          
          # Commit and push with retry
          if ! git diff --cached --quiet; then
            git commit -m "Update state [skip ci]"
            
            # Try to push, retry once if failed
            if ! git push; then
              echo "⚠️ First push failed, pulling and retrying..."
              git pull --rebase || true
              git push || echo "❌ Push failed after retry"
            fi
            
            echo "✅ State saved"
          else
            echo "No changes"
          fi
